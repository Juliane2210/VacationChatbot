# -*- coding: utf-8 -*-
"""Out of scope intent imbalanced classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eF1NcbPjEex8H87AhM-SIErLugzi1pV6

# Acknowledgement
Dataset:https://www.kaggle.com/stefanlarson/outofscope-intent-classification-dataset

Context:

Most supervised machine learning tasks assume a dataset with a set of well-defined target label set. But what happens when a trained model meets the real world, where inputs to the trained model might not be from the well-defined target label set? This dataset offers a way to evaluate intent classification models on "out-of-scope" inputs.

"Out-of-scope" inputs are those that do not belong to the set of "in-scope" target labels. You may have heard other ways of referring to out-of-scope, including "out-of-domain" or "out-of-distribution".
Content

    is_*.json: these files house the train/val/test sets for the in-scope data. There are 150 in-scope "intents" (aka classes), which include samples such as "what is my balance" (which belongs to the balance class).
    oos_*.json: these files house the train/val/test sets for the out-of-scope data. There is one out-of-scope intent: oos. Note that you don't have to use the oos_train.json data. In other words, an ML solution to the out-of-scope problem need not be trained on out-of-scope data, but it might help!

Evaluation Metrics:

The task is intent classification, which generalizes to text classification (or categorization). This is a supervised ML problem. We use two metrics to evaluate:

    In-scope accuracy is defined as #(correctly classified in-scope samples) / #(in-scope samples).
    Out-of-scope recall is defined as #(correctly classified out-of-scope samples) / #(out-of-scope samples).

Acknowledgements:

This dataset is from An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction by Larson et al., which was published in EMNLP in 2019. The GitHub page for this dataset is linked here.
Inspiration

Most supervised machine learning tasks assume a dataset with a set of well-defined target label set. But what happens when a trained model meets the real world, where inputs to the trained model might not be from the well-defined target label set? This "out-of-distribution" problem has seen lots of recent development, as researchers and practitioners in both academia and industry are observing that many ML methods struggle on out-of-distribution data in a wide variety of tasks.

# Import dependencies
"""

import re
import os
import numpy as np
import pandas as pd
from sklearn.svm import SVC
import plotly.express as px
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from imblearn.under_sampling import RandomUnderSampler
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import SGDClassifier, LogisticRegression
from sklearn.metrics import recall_score, precision_score, make_scorer
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier, RUSBoostClassifier

pd.set_option('display.max_rows', 700)
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

"""# Load and clean data"""

train = pd.read_json('.//Project//Code//Dataset//intent_data//is_train.json')
val = pd.read_json('.//Project//Code//Dataset//intent_data//is_val.json')
test = pd.read_json('.//Project//Code//Dataset//intent_data//is_test.json')
oos_train = pd.read_json(
    './/Project//Code//Dataset//intent_data//oos_train.json')
oos_val = pd.read_json('.//Project//Code//Dataset//intent_data//oos_val.json')
oos_test = pd.read_json(
    './/Project//Code//Dataset//intent_data//oos_test.json')
files = [(train, 'train'), (val, 'val'), (test, 'test'), (oos_train,
                                                          'oos_train'), (oos_val, 'oos_val'), (oos_test, 'oos_test')]
for file, name in files:
    file.columns = ['text', 'intent']
    print(f'{name} shape:{file.shape}, {name} has {train.isna().sum().sum()} null values')
in_train = train.copy()

in_train.intent.value_counts()

"""All in-scope intents are balanced, no sampling required

Check if any text sampe has email,url,mention and hashtags
"""


def ngrams_top(corpus, ngram_range, n=10):
    vec = CountVectorizer(stop_words='english',
                          ngram_range=ngram_range).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx])
                  for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    total_list = words_freq[:n]
    df = pd.DataFrame(total_list, columns=['text', 'count'])
    return df


def get_emails(x):
    email = re.findall(r'[\w\.-]+@[\w-]+\.[\w]+', str(x))
    return " ".join(email)


def get_urls(x):
    url = re.findall(
        'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\.[\w]+', str(x))
    return " ".join(url)


def get_mentions(x):
    mention = re.findall(r'(?<=@)\w+', str(x))
    return " ".join(mention)


def get_hashtags(x):
    hashtag = re.findall(r'(?<=#)\w+', str(x))
    return " ".join(hashtag)


def text_at_a_glance(df):
    res = df.apply(get_emails)
    res = res[res.values != ""]
    print("Data has {} rows with emails".format(len(res)))
    res = df.apply(get_urls)
    res = res[res.values != ""]
    print("Data has {} rows with urls".format(len(res)))
    res = df.apply(get_mentions)
    res = res[res.values != ""]
    print("Data has {} rows with mentions".format(len(res)))
    res = df.apply(get_hashtags)
    res = res[res.values != ""]
    print("Data has {} rows with hashtags".format(len(res)))


text_at_a_glance(in_train.text)

# check where hashtag has been found, punctuations will be taken care of by vectorizer()
temp = in_train.text.apply(get_hashtags)
in_train.iloc[temp[temp.values != ""].index].text

"""# Ngrams"""

# top bigrams
bigrams = ngrams_top(in_train.text, (2, 2)).sort_values(by='count')
px.bar(data_frame=bigrams, y='text', x='count', orientation='h',
       color_discrete_sequence=['#dc3912'], opacity=0.8, width=900, height=500)

# top trigrams
trigrams = ngrams_top(in_train.text, (3, 3)).sort_values(by='count')
px.bar(data_frame=trigrams, y='text', x='count', orientation='h',
       color_discrete_sequence=['#f58518'], opacity=0.8, width=900, height=500)

"""# In-scope prediction"""


def binarize(df):
    df.intent = np.where(df.intent != 'oos', 0, 1)
    return df


def vectorizer(X):
    cv = CountVectorizer(min_df=1, ngram_range=(1, 2))
    X_en = cv.fit_transform(X)
    return cv, X_en


def labelencoder(y):
    le = LabelEncoder()
    le.fit(y)
    y_enc = le.transform(y)
    return le, y_enc


def preprocess(train):
    X = train.text
    y = train.intent
    le, y = labelencoder(y)
    cv, X = vectorizer(X)
    return X, y, cv, le


def process_non_train(df, cv, le):
    X = df.text
    y = df.intent
    X = cv.transform(X)
    y = le.transform(y)
    return X, y


def get_score(clf, binary=0):
    clf.fit(X_train, y_train)
    if binary == 1:
        y_pred = clf.predict(X_test)
        return clf, clf.score(X_val, y_val), clf.score(X_test, y_test), recall_score(y_test, y_pred), precision_score(y_test, y_pred)
    elif binary == 0:
        return clf, clf.score(X_val, y_val), clf.score(X_test, y_test)


X_train, y_train, cv, le = preprocess(in_train)
X_val, y_val = process_non_train(val, cv, le)
X_test, y_test = process_non_train(test, cv, le)

"""### Evaluate data over models"""

val_scores = []
test_scores = []
names = []

models = [(KNeighborsClassifier(n_neighbors=15), 'KNN'), (SGDClassifier(), 'SGD clf'), (MultinomialNB(), 'MultinomialNB'),
          (RandomForestClassifier(), 'Random Forest'), (SVC(kernel='linear'), 'Linear SVC')]

for model, name in models:
    clf, score, test_score = get_score(model, 0)
    names.append(name)
    val_scores.append(score*100)
    test_scores.append(test_score*100)
pd.DataFrame(data=zip(val_scores, test_scores), index=names, columns=[
             'val_score', 'test_score']).style.background_gradient()

params = {
    'loss': ['squared_hinge', 'modified_huber'],
    'alpha': [0.0001, 0.001, 0.01],
    'max_iter': [250, 500, 1000],
    'validation_fraction': [0.2]
}
cv = GridSearchCV(SGDClassifier(random_state=111),
                  param_grid=params, cv=5, n_jobs=-1, verbose=2)

cv.fit(X_train, y_train)

cv.best_params_

cv.best_score_

"""# In-Scope and Out-Scope classification (binary)

To perform in and out scope classification we have to encode all samples with intent features other than 'oos' to 0 and with 'oos' to 1 which will be done by binarize()
"""

oos_plus_train = binarize(
    pd.concat([in_train, oos_train], axis=0).reset_index(drop=True))
oos_plus_val = binarize(
    pd.concat([val, oos_val], axis=0).reset_index(drop=True))
oos_plus_test = binarize(
    pd.concat([test, oos_test], axis=0).reset_index(drop=True))

oos_count = oos_plus_train.intent.value_counts()
oos_count

oos_count.plot(kind='bar')

"""The imbalance ratio between 0 and 1 is 150

This imbalance can be dealt with by using sampling, here I will be using undersampling i.e the majority class will be undersampled to the size of minority class, coupling this with ensembling will give us best results.

Resources:

https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/

https://www.kaggle.com/residentmario/undersampling-and-oversampling-imbalanced-data/
"""

X_train, y_train, cv, le = preprocess(oos_plus_train)
X_val, y_val = process_non_train(oos_plus_val, cv, le)
X_test, y_test = process_non_train(oos_plus_test, cv, le)

val_scores = []
test_scores = []
recall = []
names = []
precision = []

# (RUSBoostClassifier(base_estimator=LogisticRegression(
#       ), sampling_strategy='not minority', random_state=111), 'Random Undersampling + Adaboost'),


models = [(BalancedRandomForestClassifier(sampling_strategy='not minority', random_state=111), 'Balanced Random Forest'),

          (EasyEnsembleClassifier(n_estimators=30,  replacement=True, sampling_strategy='not minority', random_state=111), 'Easy Ensemble')]

for model, name in models:
    _, score, test_score, recall_sc, precision_sc = get_score(model, 1)
    names.append(name)
    val_scores.append(score*100)
    test_scores.append(test_score*100)
    recall.append(recall_sc*100)
    precision.append(precision_sc*100)
pd.DataFrame(data=zip(val_scores, test_scores, recall, precision), index=names,
             columns=['val_score', 'test_score', 'recall_score', 'precision_score']).style.background_gradient()

"""Balanced Random Forest:

Accuracy ðŸ˜ª

recall ðŸ¤©

precision ðŸ˜‘

Easy ensemble has provided the best performance, next step is to explore easyensemble more

Learn more about easyensemble:

X. Y. Liu, J. Wu and Z. H. Zhou, â€œExploratory Undersampling for Class-Imbalance Learning,â€ in IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 39, no. 2, pp. 539-550, April 2009.

### Tuning EasyEnsemble
"""

params = {
    'n_estimators': [30, 50, 70],

    'replacement': [False, True]
}
cv = GridSearchCV(EasyEnsembleClassifier(n_jobs=-1, sampling_strategy='not minority', random_state=111),
                  verbose=2,
                  scoring='recall',
                  param_grid=params, cv=5,
                  n_jobs=-1)
cv.fit(X_train, y_train)

cv.best_score_

cv.best_params_

"""### Tune base estimator of EasyEnsemble"""

n_estimator = [25, 50, 100]
learning_rate = [0.001, 0.0001]
result = []
for nest in n_estimator:
    for lr in learning_rate:
        eec = EasyEnsembleClassifier(n_estimators=100,
                                     replacement=True, n_jobs=-1,
                                     sampling_strategy='not minority', random_state=111)
        eec.fit(X_train, y_train)
        recall = recall_score(y_test, eec.predict(X_test))
        result.append([nest, lr, recall])

pd.DataFrame(result, columns=['n_estimators', 'learning_rate',
             'recall_score']).style.background_gradient(subset=['recall_score'])

"""# Thank you for coming by, suggest modifications if any"""
